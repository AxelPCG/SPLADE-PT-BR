{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6sQDj08qzEY"
      },
      "source": [
        "# SPLADE v2 PT-BR - Treinamento Corrigido\n",
        "\n",
        "Este notebook cont√©m todas as corre√ß√µes necess√°rias para rodar o treinamento do SPLADE em 2025, resolvendo incompatibilidades de bibliotecas (AdamW/Hydra) e depend√™ncias de arquivos.\n",
        "\n",
        "## Corre√ß√µes Aplicadas:\n",
        "1. ‚úÖ Caminhos corrigidos de `/content/` para caminhos relativos\n",
        "2. ‚úÖ Download de datasets com verifica√ß√£o de sucesso\n",
        "3. ‚úÖ Corre√ß√£o do dataloader para lidar com valores None\n",
        "4. ‚úÖ Depend√™ncias atualizadas (torch, transformers, etc.)\n",
        "\n",
        "**Aten√ß√£o:** Certifique-se de usar um Runtime com GPU (T4 ou A100) ou CPU se n√£o houver GPU dispon√≠vel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuwHWS4pqzEZ",
        "outputId": "a30465dd-8f8d-4346-eceb-9de8bc8499bd"
      },
      "outputs": [],
      "source": [
        "# 1. Instala√ß√£o de Bibliotecas\n",
        "# !pip install pytrec_eval\n",
        "# !pip install git+https://github.com/leobavila/splade.git -q\n",
        "# !pip install hydra-core --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZNtUoOCqzEa",
        "outputId": "eda3c9f6-239e-4031-adc5-65f0758f70f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Patch aplicado: bert_optim.py corrigido.\n"
          ]
        }
      ],
      "source": [
        "# 2. Clonagem e Patch de Corre√ß√£o (AdamW)\n",
        "import os\n",
        "\n",
        "# Clona o reposit√≥rio se n√£o existir\n",
        "if not os.path.exists(\"splade\"):\n",
        "    os.system(\"git clone https://github.com/leobavila/splade.git\")\n",
        "\n",
        "# Corrige o erro de importa√ß√£o do AdamW (Transformers antigo vs novo)\n",
        "file_path = \"splade/splade/optim/bert_optim.py\"\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    new_content = content.replace(\n",
        "        \"from transformers.optimization import AdamW, get_linear_schedule_with_warmup\",\n",
        "        \"from transformers import get_linear_schedule_with_warmup; from torch.optim import AdamW\"\n",
        "    )\n",
        "\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(new_content)\n",
        "    print(\"‚úÖ Patch aplicado: bert_optim.py corrigido.\")\n",
        "else:\n",
        "    print(\"‚ùå Erro: Arquivo bert_optim.py n√£o encontrado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHizHlnbqzEa",
        "outputId": "f934648f-4b7b-4f18-addd-73507308ac65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Baixando datasets p√∫blicos usando HuggingFace Hub... (Pode levar alguns minutos)\n",
            "\n",
            "üì¶ Baixando datasets mMARCO...\n",
            "‚úÖ queries_train.tsv j√° existe (39281063 bytes), pulando download.\n",
            "‚úÖ corpus.tsv j√° existe (3431011785 bytes), pulando download.\n",
            "‚úÖ triples.train.ids.small.tsv j√° existe (905211990 bytes), pulando download.\n",
            "‚úÖ Arquivos mMARCO copiados para estrutura SPLADE\n",
            "\n",
            "üì¶ Baixando datasets mRobust...\n",
            "‚úÖ mrobust queries.tsv j√° existe (28418 bytes), pulando download.\n",
            "‚úÖ mrobust corpus.tsv j√° existe (1914138316 bytes), pulando download.\n",
            "‚úÖ qrels.robust04.txt j√° existe (6543541 bytes), pulando download.\n",
            "‚úÖ Arquivos mRobust copiados para estrutura SPLADE\n",
            "\n",
            "‚úÖ Processo de download conclu√≠do.\n"
          ]
        }
      ],
      "source": [
        "# 3. Download e Prepara√ß√£o dos Datasets (mMARCO e mRobust)\n",
        "import shutil\n",
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "print(\"‚è≥ Baixando datasets p√∫blicos usando HuggingFace Hub... (Pode levar alguns minutos)\")\n",
        "\n",
        "# Criar pastas base (usando caminhos relativos ao projeto)\n",
        "data_dir = \"./data\"\n",
        "os.makedirs(f\"{data_dir}/m_marco\", exist_ok=True)\n",
        "os.makedirs(f\"{data_dir}/m_robust\", exist_ok=True)\n",
        "\n",
        "# Criar pastas de destino do SPLADE\n",
        "os.makedirs(\"splade/data/pt/triplets\", exist_ok=True)\n",
        "os.makedirs(\"splade/data/pt/val_retrieval/collection\", exist_ok=True)\n",
        "os.makedirs(\"splade/data/pt/val_retrieval/queries\", exist_ok=True)\n",
        "\n",
        "def download_from_hf(repo_id, filename, output_path, description):\n",
        "    \"\"\"Download de arquivo do HuggingFace Hub\"\"\"\n",
        "    if os.path.exists(output_path) and os.path.getsize(output_path) > 100:  # M√≠nimo 100 bytes\n",
        "        print(f\"‚úÖ {description} j√° existe ({os.path.getsize(output_path)} bytes), pulando download.\")\n",
        "        return True\n",
        "    \n",
        "    print(f\"üì• Baixando {description}...\")\n",
        "    try:\n",
        "        downloaded_path = hf_hub_download(\n",
        "            repo_id=repo_id,\n",
        "            filename=filename,\n",
        "            repo_type=\"dataset\",\n",
        "            local_dir=None\n",
        "        )\n",
        "        # Copiar para o destino desejado\n",
        "        shutil.copy(downloaded_path, output_path)\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 100:\n",
        "            print(f\"‚úÖ {description} baixado com sucesso ({os.path.getsize(output_path)} bytes)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå {description} falhou: arquivo muito pequeno ou vazio\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao baixar {description}: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- mMARCO (Treino) ---\n",
        "print(\"\\nüì¶ Baixando datasets mMARCO...\")\n",
        "download_from_hf(\n",
        "    \"unicamp-dl/mmarco\",\n",
        "    \"data/google/queries/train/portuguese_queries.train.tsv\",\n",
        "    f\"{data_dir}/m_marco/queries_train.tsv\",\n",
        "    \"queries_train.tsv\"\n",
        ")\n",
        "\n",
        "download_from_hf(\n",
        "    \"unicamp-dl/mmarco\",\n",
        "    \"data/google/collections/portuguese_collection.tsv\",\n",
        "    f\"{data_dir}/m_marco/corpus.tsv\",\n",
        "    \"corpus.tsv\"\n",
        ")\n",
        "\n",
        "download_from_hf(\n",
        "    \"unicamp-dl/mmarco\",\n",
        "    \"data/triples.train.ids.small.tsv\",\n",
        "    f\"{data_dir}/m_marco/triples.train.ids.small.tsv\",\n",
        "    \"triples.train.ids.small.tsv\"\n",
        ")\n",
        "\n",
        "# Verificar se os arquivos foram baixados corretamente\n",
        "mmarco_files = [\"queries_train.tsv\", \"corpus.tsv\", \"triples.train.ids.small.tsv\"]\n",
        "if all(os.path.exists(f\"{data_dir}/m_marco/{f}\") and os.path.getsize(f\"{data_dir}/m_marco/{f}\") > 100 \n",
        "       for f in mmarco_files):\n",
        "    # Copiar para estrutura SPLADE\n",
        "    shutil.copy(f\"{data_dir}/m_marco/corpus.tsv\", \"splade/data/pt/triplets/corpus.tsv\")\n",
        "    shutil.copy(f\"{data_dir}/m_marco/queries_train.tsv\", \"splade/data/pt/triplets/queries_train.tsv\")\n",
        "    shutil.copy(f\"{data_dir}/m_marco/triples.train.ids.small.tsv\", \"splade/data/pt/triplets/raw.tsv\")\n",
        "    print(\"‚úÖ Arquivos mMARCO copiados para estrutura SPLADE\")\n",
        "else:\n",
        "    print(\"‚ùå Erro: Alguns arquivos mMARCO n√£o foram baixados corretamente\")\n",
        "    print(\"Verificando arquivos:\")\n",
        "    for f in mmarco_files:\n",
        "        path = f\"{data_dir}/m_marco/{f}\"\n",
        "        if os.path.exists(path):\n",
        "            print(f\"  - {f}: {os.path.getsize(path)} bytes\")\n",
        "        else:\n",
        "            print(f\"  - {f}: N√ÉO ENCONTRADO\")\n",
        "\n",
        "# --- mRobust (Valida√ß√£o) ---\n",
        "print(\"\\nüì¶ Baixando datasets mRobust...\")\n",
        "download_from_hf(\n",
        "    \"unicamp-dl/mrobust\",\n",
        "    \"data/queries/portuguese_queries.tsv\",\n",
        "    f\"{data_dir}/m_robust/queries.tsv\",\n",
        "    \"mrobust queries.tsv\"\n",
        ")\n",
        "\n",
        "download_from_hf(\n",
        "    \"unicamp-dl/mrobust\",\n",
        "    \"data/collections/portuguese_collection.tsv\",\n",
        "    f\"{data_dir}/m_robust/corpus.tsv\",\n",
        "    \"mrobust corpus.tsv\"\n",
        ")\n",
        "\n",
        "download_from_hf(\n",
        "    \"unicamp-dl/mrobust\",\n",
        "    \"qrels.robust04.txt\",\n",
        "    f\"{data_dir}/m_robust/qrels.robust04.txt\",\n",
        "    \"qrels.robust04.txt\"\n",
        ")\n",
        "\n",
        "# Verificar e copiar arquivos mRobust\n",
        "mrobust_files = [\"queries.tsv\", \"corpus.tsv\", \"qrels.robust04.txt\"]\n",
        "if all(os.path.exists(f\"{data_dir}/m_robust/{f}\") and os.path.getsize(f\"{data_dir}/m_robust/{f}\") > 100 \n",
        "       for f in mrobust_files):\n",
        "    shutil.copy(f\"{data_dir}/m_robust/corpus.tsv\", \"splade/data/pt/val_retrieval/collection/raw.tsv\")\n",
        "    shutil.copy(f\"{data_dir}/m_robust/queries.tsv\", \"splade/data/pt/val_retrieval/queries/raw.tsv\")\n",
        "    print(\"‚úÖ Arquivos mRobust copiados para estrutura SPLADE\")\n",
        "else:\n",
        "    print(\"‚ùå Erro: Alguns arquivos mRobust n√£o foram baixados corretamente\")\n",
        "    print(\"Verificando arquivos:\")\n",
        "    for f in mrobust_files:\n",
        "        path = f\"{data_dir}/m_robust/{f}\"\n",
        "        if os.path.exists(path):\n",
        "            print(f\"  - {f}: {os.path.getsize(path)} bytes\")\n",
        "        else:\n",
        "            print(f\"  - {f}: N√ÉO ENCONTRADO\")\n",
        "\n",
        "print(\"\\n‚úÖ Processo de download conclu√≠do.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32EwFsPvqzEa",
        "outputId": "ff444073-7f81-4edd-e47b-3b06a87d9dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ QREL convertido para JSON.\n"
          ]
        }
      ],
      "source": [
        "# 4. Converter QRELS para JSON\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "qrel = defaultdict(dict)\n",
        "data_dir = \"./data\"\n",
        "qrel_path = f\"{data_dir}/m_robust/qrels.robust04.txt\"\n",
        "\n",
        "if os.path.exists(qrel_path):\n",
        "    with open(qrel_path, 'r') as file:\n",
        "        for line in file:\n",
        "            fields = line.split()\n",
        "            if len(fields) >= 4:\n",
        "                q_id = fields[0]\n",
        "                doc_id = fields[2]\n",
        "                rel = fields[3]\n",
        "                qrel[q_id][doc_id] = int(rel)\n",
        "\n",
        "    with open('splade/data/pt/val_retrieval/qrel.json', 'w') as file:\n",
        "        json.dump(qrel, file)\n",
        "    print(\"‚úÖ QREL convertido para JSON.\")\n",
        "else:\n",
        "    print(f\"‚ùå Erro: qrels.robust04.txt n√£o encontrado em {qrel_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7m9IhRyqzEa",
        "outputId": "2b201a33-c26f-4b39-a316-d650d9173b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configura√ß√µes recriadas com sucesso (loss: InBatchPairwiseNLL adicionado).\n"
          ]
        }
      ],
      "source": [
        "# 5. Gerar Arquivos de Configura√ß√£o (CR√çTICO: Inclus√£o do par√¢metro 'loss')\n",
        "# Corre√ß√£o: Adicionado 'loss: InBatchPairwiseNLL' para corrigir o ConfigKeyError.\n",
        "\n",
        "import os\n",
        "\n",
        "# Criar estrutura de pastas\n",
        "os.makedirs(\"splade/conf/train/config\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/train/data\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/train/model\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/index\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/retrieve_evaluate\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/flops\", exist_ok=True)\n",
        "\n",
        "# 5.1 Modelo\n",
        "with open(\"splade/conf/train/model/splade_bertimbau_base.yaml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "_target_: splade.models.transformer_rep.Splade\n",
        "# Nota: O par√¢metro real ser√° lido do init_dict abaixo\n",
        "model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
        "    \"\"\")\n",
        "\n",
        "# 5.2 Dados\n",
        "with open(\"splade/conf/train/data/pt.yaml\", \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "# @package _global_\n",
        "data:\n",
        "    type: triplets\n",
        "    TRAIN_DATA_DIR: {os.getcwd()}/splade/data/pt/triplets\n",
        "    VALIDATION_DATA_DIR: {os.getcwd()}/splade/data/pt/val_retrieval\n",
        "    QREL_PATH: {os.getcwd()}/splade/data/pt/val_retrieval/qrel.json\n",
        "    \"\"\")\n",
        "\n",
        "# 5.3 Config de Treino (CORRE√á√ÉO AQUI: Adicionado 'loss')\n",
        "with open(\"splade/conf/train/config/splade_pt.yaml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "# @package _global_\n",
        "config:\n",
        "    lr: 2e-5\n",
        "    seed: 123\n",
        "    gradient_accumulation_steps: 1\n",
        "    weight_decay: 0.01\n",
        "    validation_metrics: [MRR@10]\n",
        "    pretrained_no_yaml_config: false\n",
        "    nb_iterations: 150000\n",
        "    train_batch_size: 32\n",
        "    eval_batch_size: 32\n",
        "    index_retrieval_batch_size: 32\n",
        "    record_frequency: 1000\n",
        "    train_monitoring_freq: 500\n",
        "    warmup_steps: 6000\n",
        "    max_length: 256\n",
        "    fp16: true\n",
        "    matching_type: splade\n",
        "    monitoring_ckpt: true\n",
        "    tokenizer_type: neuralmind/bert-base-portuguese-cased\n",
        "\n",
        "    # Par√¢metro de perda que faltava:\n",
        "    loss: InBatchPairwiseNLL\n",
        "\n",
        "    # Chaves obrigat√≥rias para o Hydra:\n",
        "    checkpoint_dir: \"\"\n",
        "    index_dir: \"\"\n",
        "    out_dir: \"\"\n",
        "\n",
        "    regularization:\n",
        "        FLOPS:\n",
        "            lambda_q: 0.0003\n",
        "            lambda_d: 0.0001\n",
        "            T: 50000\n",
        "    \"\"\")\n",
        "\n",
        "# 5.4 Config Geral\n",
        "with open(\"splade/conf/config_splade_pt.yaml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "defaults:\n",
        "  - train/data: pt\n",
        "  - train/model: splade_bertimbau_base\n",
        "  - train/config: splade_pt\n",
        "  - index: pt\n",
        "  - retrieve_evaluate: pt\n",
        "  - flops: pt\n",
        "  - _self_\n",
        "\n",
        "# init_dict com as corre√ß√µes anteriores\n",
        "init_dict:\n",
        "  model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
        "  fp16: true\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/pt/out\n",
        "  job:\n",
        "    chdir: true\n",
        "    \"\"\")\n",
        "\n",
        "# 5.5 Placeholders\n",
        "with open(\"splade/conf/index/pt.yaml\", \"w\") as f: f.write(\"# Placeholder\")\n",
        "with open(\"splade/conf/retrieve_evaluate/pt.yaml\", \"w\") as f: f.write(\"# Placeholder\")\n",
        "with open(\"splade/conf/flops/pt.yaml\", \"w\") as f: f.write(\"# Placeholder\")\n",
        "\n",
        "print(\"‚úÖ Configura√ß√µes recriadas com sucesso (loss: InBatchPairwiseNLL adicionado).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZGCzclpCqzEa",
        "outputId": "4e9da717-5ada-4375-d9b9-b0c7e45c4267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Iniciando treinamento... Acompanhe os logs abaixo.\n",
            "Nota: Ignorar avisos 'Unable to register cuFFT/cuDNN' do TensorFlow/JAX.\n",
            "================================================================================\n",
            "\n",
            "/home/user/Projects/SPLADE-PT-BR/splade/splade/train_from_triplets_ids.py:20: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=CONFIG_PATH, config_name=CONFIG_NAME)\n",
            "data:\n",
            "  type: triplets\n",
            "  TRAIN_DATA_DIR: /home/user/Projects/SPLADE-PT-BR/splade/data/pt/triplets\n",
            "  VALIDATION_DATA_DIR: /home/user/Projects/SPLADE-PT-BR/splade/data/pt/val_retrieval\n",
            "  QREL_PATH: /home/user/Projects/SPLADE-PT-BR/splade/data/pt/val_retrieval/qrel.json\n",
            "train:\n",
            "  model:\n",
            "    _target_: splade.models.transformer_rep.Splade\n",
            "    model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
            "config:\n",
            "  lr: 2.0e-05\n",
            "  seed: 123\n",
            "  gradient_accumulation_steps: 4\n",
            "  weight_decay: 0.01\n",
            "  validation_metrics:\n",
            "  - MRR@10\n",
            "  pretrained_no_yaml_config: false\n",
            "  nb_iterations: 150000\n",
            "  train_batch_size: 8\n",
            "  eval_batch_size: 16\n",
            "  index_retrieval_batch_size: 16\n",
            "  record_frequency: 1000\n",
            "  train_monitoring_freq: 500\n",
            "  warmup_steps: 6000\n",
            "  max_length: 256\n",
            "  fp16: true\n",
            "  matching_type: splade\n",
            "  monitoring_ckpt: true\n",
            "  tokenizer_type: neuralmind/bert-base-portuguese-cased\n",
            "  loss: InBatchPairwiseNLL\n",
            "  checkpoint_dir: experiments/pt/checkpoint\n",
            "  index_dir: experiments/pt/index\n",
            "  out_dir: experiments/pt/out\n",
            "  regularization:\n",
            "    FLOPS:\n",
            "      lambda_q: 0.0003\n",
            "      lambda_d: 0.0001\n",
            "      T: 50000\n",
            "index: {}\n",
            "retrieve_evaluate: {}\n",
            "flops: {}\n",
            "init_dict:\n",
            "  model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
            "  fp16: true\n",
            "\n",
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "üöÄ Usando GPU: NVIDIA GeForce RTX 3060\n",
            "üíæ Mem√≥ria GPU dispon√≠vel: 12.49 GB\n",
            "üßπ Cache CUDA limpo\n",
            "@@@@ RESUMING TRAINING @@@\n",
            "WARNING: change seed to change data order when restoring !\n",
            "starting from step 1000\n",
            "149001 remaining iterations\n",
            "restoring model: Splade\n",
            "loading regularizer\n",
            "üìñ Criando mapeamento lazy de documentos (economiza RAM)...\n",
            "‚úÖ Mapeador de documentos criado (lazy loading)\n",
            "üìñ Criando mapeamento lazy de queries (economiza RAM)...\n",
            "‚úÖ Mapeador de queries criado (lazy loading)\n",
            "üì¶ Carregando dataset de triplas...\n",
            "üì¶ Indexando dataset (lazy loading para economizar mem√≥ria RAM)...\n",
            "Loading from: /home/user/Projects/SPLADE-PT-BR/splade/data/pt/triplets/raw.tsv\n",
            "\n",
            "Indexing triplets: 0 lines [00:00, ? lines/s]\n",
            "Indexing triplets: 2311586 lines [00:01, 2311570.57 lines/s]\n",
            "Indexing triplets: 4901733 lines [00:02, 2475432.36 lines/s]\n",
            "Indexing triplets: 7461523 lines [00:03, 2513946.25 lines/s]\n",
            "Indexing triplets: 10014756 lines [00:04, 2529450.93 lines/s]\n",
            "Indexing triplets: 12618194 lines [00:05, 2556126.72 lines/s]\n",
            "Indexing triplets: 15248233 lines [00:06, 2581252.06 lines/s]\n",
            "Indexing triplets: 17864539 lines [00:07, 2592708.34 lines/s]\n",
            "Indexing triplets: 20499894 lines [00:08, 2606280.61 lines/s]\n",
            "Indexing triplets: 23109696 lines [00:09, 2607379.50 lines/s]\n",
            "Indexing triplets: 25762630 lines [00:10, 2621441.91 lines/s]\n",
            "Indexing triplets: 28384072 lines [00:11, 2615433.04 lines/s]\n",
            "Indexing triplets: 31045765 lines [00:12, 2629481.05 lines/s]\n",
            "Indexing triplets: 33699250 lines [00:13, 2636742.68 lines/s]\n",
            "Indexing triplets: 36374103 lines [00:14, 2648242.59 lines/s]\n",
            "Indexing triplets: 39022382 lines [00:15, 2647934.03 lines/s]\n",
            "Indexing triplets: 39780811 lines [00:15, 2601061.29 lines/s]\n",
            "‚úÖ Dataset indexado: 39,780,811 triplas (lazy loading ativado)\n",
            "üíæ Economia de mem√≥ria: ~7956.2 MB n√£o carregados na RAM\n",
            "+++++ BEGIN TRAINING +++++\n",
            "initialize trainer...\n",
            " --- total number parameters: 108954466\n",
            " === trainer config === \n",
            " ========================= {'lr': 2e-05, 'seed': 123, 'gradient_accumulation_steps': 4, 'weight_decay': 0.01, 'validation_metrics': ['MRR@10'], 'pretrained_no_yaml_config': False, 'nb_iterations': 150000, 'train_batch_size': 8, 'eval_batch_size': 16, 'index_retrieval_batch_size': 16, 'record_frequency': 1000, 'train_monitoring_freq': 500, 'warmup_steps': 6000, 'max_length': 256, 'fp16': True, 'matching_type': 'splade', 'monitoring_ckpt': True, 'tokenizer_type': 'neuralmind/bert-base-portuguese-cased', 'loss': 'InBatchPairwiseNLL', 'checkpoint_dir': 'experiments/pt/checkpoint', 'index_dir': 'experiments/pt/index', 'out_dir': 'experiments/pt/out', 'regularization': {'FLOPS': {'lambda_q': 0.0003, 'lambda_d': 0.0001, 'T': 50000}}}\n",
            "Using FP16: True\n",
            "/home/user/Projects/SPLADE-PT-BR/splade/splade/tasks/amp.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n",
            "üìñ Indexando queries_train.tsv...\n",
            "üìñ Indexando queries_train.tsv...\n",
            "üìñ Indexando queries_train.tsv...\n",
            "üìñ Indexando queries_train.tsv...\n",
            "\n",
            "Indexing queries_train.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing queries_train.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing queries_train.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing queries_train.tsv: 0 lines [00:00, ? lines/s]\n",
            "Training:   1%|          | 1001/150000 [00:00<?, ? iter/s]\n",
            "Indexing queries_train.tsv: 520688 lines [00:00, 1041361.60 lines/s]\n",
            "Indexing queries_train.tsv: 532474 lines [00:00, 1064933.27 lines/s]\n",
            "Indexing queries_train.tsv: 496905 lines [00:00, 993791.99 lines/s]\n",
            "Indexing queries_train.tsv: 509217 lines [00:00, 1018419.92 lines/s]\n",
            "Indexing queries_train.tsv: 808731 lines [00:00, 1038200.49 lines/s]\n",
            "‚úÖ 808,731 IDs indexados\n",
            "üìñ Indexando corpus.tsv...\n",
            "\n",
            "Indexing corpus.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing queries_train.tsv: 808731 lines [00:00, 1010623.76 lines/s]\n",
            "‚úÖ 808,731 IDs indexados\n",
            "üìñ Indexando corpus.tsv...\n",
            "\n",
            "Indexing corpus.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing queries_train.tsv: 808731 lines [00:00, 1004353.60 lines/s]\n",
            "‚úÖ 808,731 IDs indexados\n",
            "üìñ Indexando corpus.tsv...\n",
            "\n",
            "Indexing corpus.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing queries_train.tsv: 808731 lines [00:00, 975290.61 lines/s]\n",
            "‚úÖ 808,731 IDs indexados\n",
            "üìñ Indexando corpus.tsv...\n",
            "\n",
            "Indexing corpus.tsv: 0 lines [00:00, ? lines/s]\n",
            "Indexing corpus.tsv: 331020 lines [00:00, 662034.63 lines/s]\n",
            "Indexing corpus.tsv: 324992 lines [00:00, 649980.90 lines/s]\n",
            "Indexing corpus.tsv: 328778 lines [00:00, 657552.24 lines/s]\n",
            "Indexing corpus.tsv: 330316 lines [00:00, 660627.59 lines/s]\n",
            "Indexing corpus.tsv: 662038 lines [00:01, 658747.95 lines/s]\n",
            "Indexing corpus.tsv: 649983 lines [00:01, 647502.81 lines/s]\n",
            "Indexing corpus.tsv: 657555 lines [00:01, 652017.69 lines/s]\n",
            "Indexing corpus.tsv: 660630 lines [00:01, 657844.72 lines/s]\n",
            "Indexing corpus.tsv: 991418 lines [00:01, 645080.37 lines/s]\n",
            "Indexing corpus.tsv: 973738 lines [00:01, 631302.95 lines/s]\n",
            "Indexing corpus.tsv: 983581 lines [00:01, 635734.95 lines/s]\n",
            "Indexing corpus.tsv: 989557 lines [00:01, 642469.57 lines/s]\n",
            "Indexing corpus.tsv: 1314109 lines [00:02, 642032.46 lines/s]\n",
            "Indexing corpus.tsv: 1289610 lines [00:02, 626886.39 lines/s]\n",
            "Indexing corpus.tsv: 1301658 lines [00:02, 634538.06 lines/s]\n",
            "Indexing corpus.tsv: 1310986 lines [00:02, 640159.81 lines/s]\n",
            "Indexing corpus.tsv: 1635207 lines [00:02, 620278.17 lines/s]\n",
            "Indexing corpus.tsv: 1603175 lines [00:02, 601730.54 lines/s]\n",
            "Indexing corpus.tsv: 1619045 lines [00:02, 605979.56 lines/s]\n",
            "Indexing corpus.tsv: 1631172 lines [00:02, 611832.70 lines/s]\n",
            "Indexing corpus.tsv: 1962824 lines [00:03, 631796.27 lines/s]\n",
            "Indexing corpus.tsv: 1928172 lines [00:03, 617532.53 lines/s]\n",
            "Indexing corpus.tsv: 1946221 lines [00:03, 621788.29 lines/s]\n",
            "Indexing corpus.tsv: 1958674 lines [00:03, 625953.54 lines/s]\n",
            "Indexing corpus.tsv: 2281935 lines [00:03, 633852.29 lines/s]\n",
            "Indexing corpus.tsv: 2245645 lines [00:03, 623080.70 lines/s]\n",
            "Indexing corpus.tsv: 2263861 lines [00:03, 626081.38 lines/s]\n",
            "Indexing corpus.tsv: 2278254 lines [00:03, 630158.09 lines/s]\n",
            "Indexing corpus.tsv: 2599252 lines [00:04, 633481.20 lines/s]\n",
            "Indexing corpus.tsv: 2559404 lines [00:04, 624467.78 lines/s]\n",
            "Indexing corpus.tsv: 2577913 lines [00:04, 626712.76 lines/s]\n",
            "Indexing corpus.tsv: 2594000 lines [00:04, 625299.87 lines/s]\n",
            "Indexing corpus.tsv: 2916257 lines [00:04, 591075.04 lines/s]\n",
            "Indexing corpus.tsv: 2872066 lines [00:04, 574187.86 lines/s]\n",
            "Indexing corpus.tsv: 2891758 lines [00:04, 575887.45 lines/s]\n",
            "Indexing corpus.tsv: 2907105 lines [00:04, 577317.03 lines/s]\n",
            "Indexing corpus.tsv: 3243499 lines [00:05, 609581.67 lines/s]\n",
            "Indexing corpus.tsv: 3196432 lines [00:05, 595695.60 lines/s]\n",
            "Indexing corpus.tsv: 3217104 lines [00:05, 597450.36 lines/s]\n",
            "Indexing corpus.tsv: 3234045 lines [00:05, 599375.07 lines/s]\n",
            "Indexing corpus.tsv: 3568709 lines [00:05, 621597.74 lines/s]\n",
            "Indexing corpus.tsv: 3518307 lines [00:05, 609720.90 lines/s]\n",
            "Indexing corpus.tsv: 3540544 lines [00:05, 611867.68 lines/s]\n",
            "Indexing corpus.tsv: 3557504 lines [00:05, 613236.03 lines/s]\n",
            "Indexing corpus.tsv: 3881439 lines [00:06, 621843.59 lines/s]\n",
            "Indexing corpus.tsv: 3831130 lines [00:06, 614406.24 lines/s]\n",
            "Indexing corpus.tsv: 3849276 lines [00:06, 609454.62 lines/s]\n",
            "Indexing corpus.tsv: 3866691 lines [00:06, 608290.07 lines/s]\n",
            "Indexing corpus.tsv: 4201311 lines [00:06, 627156.12 lines/s]\n",
            "Indexing corpus.tsv: 4147252 lines [00:06, 619684.91 lines/s]\n",
            "Indexing corpus.tsv: 4166646 lines [00:06, 616900.21 lines/s]\n",
            "Indexing corpus.tsv: 4183931 lines [00:06, 615979.08 lines/s]\n",
            "Indexing corpus.tsv: 4515882 lines [00:07, 626149.02 lines/s]\n",
            "Indexing corpus.tsv: 4458536 lines [00:07, 616115.58 lines/s]\n",
            "Indexing corpus.tsv: 4477838 lines [00:07, 618523.21 lines/s]\n",
            "Indexing corpus.tsv: 4494707 lines [00:07, 617625.03 lines/s]\n",
            "Indexing corpus.tsv: 4830320 lines [00:07, 626959.93 lines/s]\n",
            "Indexing corpus.tsv: 4770079 lines [00:07, 618181.36 lines/s]\n",
            "Indexing corpus.tsv: 4790149 lines [00:07, 620335.43 lines/s]\n",
            "Indexing corpus.tsv: 4807128 lines [00:07, 619766.43 lines/s]\n",
            "Indexing corpus.tsv: 5144286 lines [00:08, 622310.75 lines/s]\n",
            "Indexing corpus.tsv: 5079890 lines [00:08, 614876.99 lines/s]\n",
            "Indexing corpus.tsv: 5101047 lines [00:08, 617608.14 lines/s]\n",
            "Indexing corpus.tsv: 5117700 lines [00:08, 612382.12 lines/s]\n",
            "Indexing corpus.tsv: 5455805 lines [00:08, 620591.59 lines/s]\n",
            "Indexing corpus.tsv: 5387837 lines [00:08, 613269.87 lines/s]\n",
            "Indexing corpus.tsv: 5410365 lines [00:08, 615528.12 lines/s]\n",
            "Indexing corpus.tsv: 5425092 lines [00:08, 613091.47 lines/s]\n",
            "Indexing corpus.tsv: 5766352 lines [00:09, 533482.72 lines/s]\n",
            "Indexing corpus.tsv: 5694823 lines [00:09, 518206.48 lines/s]\n",
            "Indexing corpus.tsv: 5718487 lines [00:09, 520413.43 lines/s]\n",
            "Indexing corpus.tsv: 5732028 lines [00:09, 519223.93 lines/s]\n",
            "Indexing corpus.tsv: 6061930 lines [00:10, 548910.41 lines/s]\n",
            "Indexing corpus.tsv: 6011854 lines [00:10, 548792.05 lines/s]\n",
            "Indexing corpus.tsv: 6037045 lines [00:10, 551254.24 lines/s]\n",
            "Indexing corpus.tsv: 6048987 lines [00:10, 549536.53 lines/s]\n",
            "Indexing corpus.tsv: 6344818 lines [00:10, 549289.80 lines/s]\n",
            "Indexing corpus.tsv: 6310112 lines [00:10, 561860.78 lines/s]\n",
            "Indexing corpus.tsv: 6338553 lines [00:10, 565439.58 lines/s]\n",
            "Indexing corpus.tsv: 6349694 lines [00:10, 563754.48 lines/s]\n",
            "Indexing corpus.tsv: 6639061 lines [00:11, 560326.02 lines/s]\n",
            "Indexing corpus.tsv: 6621777 lines [00:11, 579145.40 lines/s]\n",
            "Indexing corpus.tsv: 6654142 lines [00:11, 583935.00 lines/s]\n",
            "Indexing corpus.tsv: 6661575 lines [00:11, 580638.35 lines/s]\n",
            "Indexing corpus.tsv: 6925901 lines [00:11, 564150.91 lines/s]\n",
            "Indexing corpus.tsv: 6927012 lines [00:11, 588121.54 lines/s]\n",
            "Indexing corpus.tsv: 6962340 lines [00:11, 593241.09 lines/s]\n",
            "Indexing corpus.tsv: 6968231 lines [00:11, 590004.04 lines/s]\n",
            "Indexing corpus.tsv: 7220418 lines [00:12, 571383.04 lines/s]\n",
            "Indexing corpus.tsv: 7239173 lines [00:12, 598636.39 lines/s]\n",
            "Indexing corpus.tsv: 7277622 lines [00:12, 604085.01 lines/s]\n",
            "Indexing corpus.tsv: 7283247 lines [00:12, 601634.15 lines/s]\n",
            "Indexing corpus.tsv: 7521508 lines [00:12, 580417.92 lines/s]\n",
            "Indexing corpus.tsv: 7548506 lines [00:12, 604509.62 lines/s]\n",
            "Indexing corpus.tsv: 7583155 lines [00:12, 602949.67 lines/s]\n",
            "Indexing corpus.tsv: 7591885 lines [00:12, 606220.82 lines/s]\n",
            "Indexing corpus.tsv: 7813488 lines [00:13, 577346.04 lines/s]\n",
            "Indexing corpus.tsv: 7853243 lines [00:13, 600308.35 lines/s]\n",
            "Indexing corpus.tsv: 7887067 lines [00:13, 591628.34 lines/s]\n",
            "Indexing corpus.tsv: 7897446 lines [00:13, 597824.60 lines/s]\n",
            "Indexing corpus.tsv: 8111595 lines [00:13, 582916.00 lines/s]\n",
            "Indexing corpus.tsv: 8156949 lines [00:13, 602216.29 lines/s]\n",
            "Indexing corpus.tsv: 8185432 lines [00:13, 593114.72 lines/s]\n",
            "Indexing corpus.tsv: 8198165 lines [00:13, 597798.95 lines/s]\n",
            "Indexing corpus.tsv: 8406030 lines [00:14, 584681.17 lines/s]\n",
            "Indexing corpus.tsv: 8459299 lines [00:14, 602301.96 lines/s]\n",
            "Indexing corpus.tsv: 8483322 lines [00:14, 591678.65 lines/s]\n",
            "Indexing corpus.tsv: 8498329 lines [00:14, 594740.99 lines/s]\n",
            "Indexing corpus.tsv: 8699021 lines [00:14, 582205.17 lines/s]\n",
            "Indexing corpus.tsv: 8761320 lines [00:14, 595548.15 lines/s]\n",
            "Indexing corpus.tsv: 8780086 lines [00:14, 589092.03 lines/s]\n",
            "Indexing corpus.tsv: 8796582 lines [00:14, 591104.26 lines/s]\n",
            "Indexing corpus.tsv: 8841823 lines [00:14, 598248.27 lines/s]\n",
            "‚úÖ 8,841,823 IDs indexados\n",
            "\n",
            "Indexing corpus.tsv: 8841823 lines [00:14, 599318.49 lines/s]\n",
            "‚úÖ 8,841,823 IDs indexados\n",
            "\n",
            "Indexing corpus.tsv: 8841823 lines [00:14, 599099.46 lines/s]\n",
            "‚úÖ 8,841,823 IDs indexados\n",
            "\n",
            "Indexing corpus.tsv: 8841823 lines [00:14, 600083.94 lines/s]\n",
            "‚úÖ 8,841,823 IDs indexados\n",
            "/home/user/Projects/SPLADE-PT-BR/splade/splade/tasks/amp.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "/home/user/Projects/SPLADE-PT-BR/splade/splade/tasks/transformer_trainer.py:222: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast() if self.fp16 else amp.NullContextManager():\n",
            "/home/user/Projects/SPLADE-PT-BR/splade/splade/models/transformer_rep.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n",
            "/home/user/Projects/SPLADE-PT-BR/splade/splade/models/transformer_rep.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n",
            "\n",
            "Training:   1%|          | 1002/150000 [00:16<688:04:09, 16.62s/ iter]\n",
            "Training:   1%|          | 1008/150000 [00:17<78:36:46,  1.90s/ iter] \n",
            "Training:   1%|          | 1008/150000 [00:18<78:36:46,  1.90s/ iter, loss=79.4159, avg_loss=37.0595, lr=8.40e-07]\n",
            "Training:   1%|          | 1014/150000 [00:18<39:23:12,  1.05 iter/s, loss=79.4159, avg_loss=37.0595, lr=8.40e-07]\n",
            "Training:   1%|          | 1020/150000 [00:20<25:17:33,  1.64 iter/s, loss=79.4159, avg_loss=37.0595, lr=8.40e-07]\n",
            "Training:   1%|          | 1020/150000 [00:20<25:17:33,  1.64 iter/s, loss=67.0383, avg_loss=52.3454, lr=8.50e-07]\n",
            "Training:   1%|          | 1026/150000 [00:21<18:36:02,  2.22 iter/s, loss=67.0383, avg_loss=52.3454, lr=8.50e-07]\n",
            "Training:   1%|          | 1026/150000 [00:22<18:36:02,  2.22 iter/s, loss=192.5691, avg_loss=78.3531, lr=8.57e-07]\n",
            "Training:   1%|          | 1032/150000 [00:22<14:57:41,  2.77 iter/s, loss=192.5691, avg_loss=78.3531, lr=8.57e-07]\n",
            "Training:   1%|          | 1038/150000 [00:23<12:48:16,  3.23 iter/s, loss=192.5691, avg_loss=78.3531, lr=8.57e-07]\n",
            "Training:   1%|          | 1038/150000 [00:24<12:48:16,  3.23 iter/s, loss=71.3040, avg_loss=92.7281, lr=8.67e-07] \n",
            "Training:   1%|          | 1044/150000 [00:24<11:01:02,  3.76 iter/s, loss=71.3040, avg_loss=92.7281, lr=8.67e-07]\n",
            "Training:   1%|          | 1050/150000 [00:25<10:05:10,  4.10 iter/s, loss=71.3040, avg_loss=92.7281, lr=8.67e-07]\n",
            "Training:   1%|          | 1050/150000 [00:26<10:05:10,  4.10 iter/s, loss=133.8876, avg_loss=112.4526, lr=8.73e-07]\n",
            "Training:   1%|          | 1056/150000 [00:27<9:18:49,  4.44 iter/s, loss=133.8876, avg_loss=112.4526, lr=8.73e-07] \n",
            "Training:   1%|          | 1056/150000 [00:28<9:18:49,  4.44 iter/s, loss=25.1104, avg_loss=118.4037, lr=8.83e-07] \n",
            "Training:   1%|          | 1062/150000 [00:28<9:05:10,  4.55 iter/s, loss=25.1104, avg_loss=118.4037, lr=8.83e-07]\n",
            "Training:   1%|          | 1068/150000 [00:29<8:43:26,  4.74 iter/s, loss=25.1104, avg_loss=118.4037, lr=8.83e-07]\n",
            "Training:   1%|          | 1068/150000 [00:30<8:43:26,  4.74 iter/s, loss=59.4874, avg_loss=137.0085, lr=8.90e-07]\n",
            "Training:   1%|          | 1074/150000 [00:30<8:28:31,  4.88 iter/s, loss=59.4874, avg_loss=137.0085, lr=8.90e-07]\n",
            "Training:   1%|          | 1080/150000 [00:31<8:12:36,  5.04 iter/s, loss=59.4874, avg_loss=137.0085, lr=8.90e-07]\n",
            "Training:   1%|          | 1080/150000 [00:31<8:12:36,  5.04 iter/s, loss=68.8381, avg_loss=148.4803, lr=9.00e-07]\n",
            "Training:   1%|          | 1086/150000 [00:32<8:04:56,  5.12 iter/s, loss=68.8381, avg_loss=148.4803, lr=9.00e-07]\n",
            "Training:   1%|          | 1086/150000 [00:33<8:04:56,  5.12 iter/s, loss=28.8240, avg_loss=155.8207, lr=9.07e-07]\n",
            "Training:   1%|          | 1092/150000 [00:33<7:59:15,  5.18 iter/s, loss=28.8240, avg_loss=155.8207, lr=9.07e-07]\n",
            "Training:   1%|          | 1098/150000 [00:35<8:01:06,  5.16 iter/s, loss=28.8240, avg_loss=155.8207, lr=9.07e-07]\n",
            "Training:   1%|          | 1098/150000 [00:35<8:01:06,  5.16 iter/s, loss=116.4237, avg_loss=171.7226, lr=9.17e-07]\n",
            "Training:   1%|          | 1104/150000 [00:36<8:22:40,  4.94 iter/s, loss=116.4237, avg_loss=171.7226, lr=9.17e-07]\n",
            "Training:   1%|          | 1110/150000 [00:37<8:14:32,  5.02 iter/s, loss=116.4237, avg_loss=171.7226, lr=9.17e-07]\n",
            "Training:   1%|          | 1110/150000 [00:37<8:14:32,  5.02 iter/s, loss=54.7900, avg_loss=182.7335, lr=9.23e-07] \n",
            "Training:   1%|          | 1116/150000 [00:38<8:16:39,  5.00 iter/s, loss=54.7900, avg_loss=182.7335, lr=9.23e-07]\n",
            "Training:   1%|          | 1116/150000 [00:39<8:16:39,  5.00 iter/s, loss=67.7816, avg_loss=188.2656, lr=9.33e-07]\n",
            "Training:   1%|          | 1122/150000 [00:39<8:09:21,  5.07 iter/s, loss=67.7816, avg_loss=188.2656, lr=9.33e-07]\n",
            "Training:   1%|          | 1128/150000 [00:41<8:02:19,  5.14 iter/s, loss=67.7816, avg_loss=188.2656, lr=9.33e-07]\n",
            "Training:   1%|          | 1128/150000 [00:41<8:02:19,  5.14 iter/s, loss=54.4322, avg_loss=191.5647, lr=9.40e-07]\n",
            "Training:   1%|          | 1134/150000 [00:42<7:59:05,  5.18 iter/s, loss=54.4322, avg_loss=191.5647, lr=9.40e-07]\n",
            "Training:   1%|          | 1140/150000 [00:43<7:50:05,  5.28 iter/s, loss=54.4322, avg_loss=191.5647, lr=9.40e-07]\n",
            "Training:   1%|          | 1140/150000 [00:43<7:50:05,  5.28 iter/s, loss=82.4296, avg_loss=193.2448, lr=9.50e-07]\n",
            "Training:   1%|          | 1146/150000 [00:44<8:00:11,  5.17 iter/s, loss=82.4296, avg_loss=193.2448, lr=9.50e-07]\n",
            "Training:   1%|          | 1146/150000 [00:45<8:00:11,  5.17 iter/s, loss=151.0262, avg_loss=192.5808, lr=9.57e-07]\n",
            "Training:   1%|          | 1152/150000 [00:45<7:56:18,  5.21 iter/s, loss=151.0262, avg_loss=192.5808, lr=9.57e-07]\n",
            "Training:   1%|          | 1158/150000 [00:46<8:03:27,  5.13 iter/s, loss=151.0262, avg_loss=192.5808, lr=9.57e-07]\n",
            "Training:   1%|          | 1158/150000 [00:47<8:03:27,  5.13 iter/s, loss=42.9051, avg_loss=199.0124, lr=9.67e-07] \n",
            "Training:   1%|          | 1164/150000 [00:48<8:06:16,  5.10 iter/s, loss=42.9051, avg_loss=199.0124, lr=9.67e-07]\n",
            "Training:   1%|          | 1170/150000 [00:49<8:08:25,  5.08 iter/s, loss=42.9051, avg_loss=199.0124, lr=9.67e-07]\n",
            "Training:   1%|          | 1170/150000 [00:49<8:08:25,  5.08 iter/s, loss=59.0116, avg_loss=197.3229, lr=9.73e-07]\n",
            "Training:   1%|          | 1176/150000 [00:50<8:03:35,  5.13 iter/s, loss=59.0116, avg_loss=197.3229, lr=9.73e-07]\n",
            "Training:   1%|          | 1176/150000 [00:51<8:03:35,  5.13 iter/s, loss=42.7181, avg_loss=205.0185, lr=9.83e-07]\n",
            "Training:   1%|          | 1182/150000 [00:51<8:04:35,  5.12 iter/s, loss=42.7181, avg_loss=205.0185, lr=9.83e-07]\n",
            "Training:   1%|          | 1188/150000 [00:52<8:05:09,  5.11 iter/s, loss=42.7181, avg_loss=205.0185, lr=9.83e-07]\n",
            "Training:   1%|          | 1188/150000 [00:53<8:05:09,  5.11 iter/s, loss=18.2566, avg_loss=203.0296, lr=9.90e-07]\n",
            "Training:   1%|          | 1194/150000 [00:53<8:04:36,  5.12 iter/s, loss=18.2566, avg_loss=203.0296, lr=9.90e-07]\n",
            "Training:   1%|          | 1200/150000 [00:55<8:07:50,  5.08 iter/s, loss=18.2566, avg_loss=203.0296, lr=9.90e-07]\n",
            "Training:   1%|          | 1200/150000 [00:55<8:07:50,  5.08 iter/s, loss=57.5948, avg_loss=206.2983, lr=1.00e-06]\n",
            "Training:   1%|          | 1206/150000 [00:56<8:33:01,  4.83 iter/s, loss=57.5948, avg_loss=206.2983, lr=1.00e-06]\n",
            "Training:   1%|          | 1206/150000 [00:57<8:33:01,  4.83 iter/s, loss=48.4991, avg_loss=201.4347, lr=1.01e-06]\n",
            "Training:   1%|          | 1212/150000 [00:57<8:21:02,  4.95 iter/s, loss=48.4991, avg_loss=201.4347, lr=1.01e-06]\n",
            "Training:   1%|          | 1217/150000 [00:58<8:20:12,  4.96 iter/s, loss=48.4991, avg_loss=201.4347, lr=1.01e-06]\n",
            "Training:   1%|          | 1217/150000 [00:59<8:20:12,  4.96 iter/s, loss=74.6199, avg_loss=205.0666, lr=1.02e-06]\n",
            "Training:   1%|          | 1222/150000 [00:59<8:19:13,  4.97 iter/s, loss=74.6199, avg_loss=205.0666, lr=1.02e-06]\n",
            "Training:   1%|          | 1228/150000 [01:00<8:11:24,  5.05 iter/s, loss=74.6199, avg_loss=205.0666, lr=1.02e-06]\n",
            "Training:   1%|          | 1228/150000 [01:01<8:11:24,  5.05 iter/s, loss=32.1513, avg_loss=208.6799, lr=1.02e-06]\n",
            "Training:   1%|          | 1234/150000 [01:01<8:00:36,  5.16 iter/s, loss=32.1513, avg_loss=208.6799, lr=1.02e-06]\n",
            "Training:   1%|          | 1240/150000 [01:03<7:53:06,  5.24 iter/s, loss=32.1513, avg_loss=208.6799, lr=1.02e-06]\n",
            "Training:   1%|          | 1240/150000 [01:03<7:53:06,  5.24 iter/s, loss=29.5302, avg_loss=202.7442, lr=1.03e-06]\n",
            "Training:   1%|          | 1246/150000 [01:04<7:59:13,  5.17 iter/s, loss=29.5302, avg_loss=202.7442, lr=1.03e-06]\n",
            "Training:   1%|          | 1246/150000 [01:05<7:59:13,  5.17 iter/s, loss=14.0548, avg_loss=203.1754, lr=1.04e-06]\n",
            "Training:   1%|          | 1252/150000 [01:05<7:53:21,  5.24 iter/s, loss=14.0548, avg_loss=203.1754, lr=1.04e-06]\n",
            "Training:   1%|          | 1258/150000 [01:06<8:02:36,  5.14 iter/s, loss=14.0548, avg_loss=203.1754, lr=1.04e-06]\n",
            "Training:   1%|          | 1258/150000 [01:07<8:02:36,  5.14 iter/s, loss=12.6673, avg_loss=204.6630, lr=1.05e-06]\n",
            "Training:   1%|          | 1264/150000 [01:07<7:58:22,  5.18 iter/s, loss=12.6673, avg_loss=204.6630, lr=1.05e-06]\n",
            "Training:   1%|          | 1270/150000 [01:08<8:03:04,  5.13 iter/s, loss=12.6673, avg_loss=204.6630, lr=1.05e-06]\n",
            "Training:   1%|          | 1270/150000 [01:09<8:03:04,  5.13 iter/s, loss=34.0661, avg_loss=201.0972, lr=1.06e-06]\n",
            "Training:   1%|          | 1276/150000 [01:10<8:00:57,  5.15 iter/s, loss=34.0661, avg_loss=201.0972, lr=1.06e-06]\n",
            "Training:   1%|          | 1276/150000 [01:11<8:00:57,  5.15 iter/s, loss=16.6210, avg_loss=202.4236, lr=1.07e-06]\n",
            "Training:   1%|          | 1282/150000 [01:11<7:59:35,  5.17 iter/s, loss=16.6210, avg_loss=202.4236, lr=1.07e-06]\n",
            "Training:   1%|          | 1288/150000 [01:12<8:04:33,  5.11 iter/s, loss=16.6210, avg_loss=202.4236, lr=1.07e-06]\n",
            "Training:   1%|          | 1288/150000 [01:12<8:04:33,  5.11 iter/s, loss=46.3422, avg_loss=197.3612, lr=1.07e-06]\n",
            "Training:   1%|          | 1294/150000 [01:13<7:54:10,  5.23 iter/s, loss=46.3422, avg_loss=197.3612, lr=1.07e-06]\n",
            "Training:   1%|          | 1300/150000 [01:14<7:53:09,  5.24 iter/s, loss=46.3422, avg_loss=197.3612, lr=1.07e-06]\n",
            "Training:   1%|          | 1300/150000 [01:15<7:53:09,  5.24 iter/s, loss=67.5283, avg_loss=194.3813, lr=1.08e-06]\n",
            "Training:   1%|          | 1306/150000 [01:15<8:13:29,  5.02 iter/s, loss=67.5283, avg_loss=194.3813, lr=1.08e-06]\n",
            "Training:   1%|          | 1306/150000 [01:16<8:13:29,  5.02 iter/s, loss=22.1489, avg_loss=192.6734, lr=1.09e-06]\n",
            "Training:   1%|          | 1312/150000 [01:17<8:05:48,  5.10 iter/s, loss=22.1489, avg_loss=192.6734, lr=1.09e-06]\n",
            "Training:   1%|          | 1318/150000 [01:18<8:12:07,  5.04 iter/s, loss=22.1489, avg_loss=192.6734, lr=1.09e-06]\n",
            "Training:   1%|          | 1318/150000 [01:18<8:12:07,  5.04 iter/s, loss=68.2491, avg_loss=187.7906, lr=1.10e-06]\n",
            "Training:   1%|          | 1324/150000 [01:19<8:19:35,  4.96 iter/s, loss=68.2491, avg_loss=187.7906, lr=1.10e-06]\n",
            "Training:   1%|          | 1329/150000 [01:20<8:19:01,  4.97 iter/s, loss=68.2491, avg_loss=187.7906, lr=1.10e-06]\n",
            "Training:   1%|          | 1329/150000 [01:20<8:19:01,  4.97 iter/s, loss=43.3630, avg_loss=183.2895, lr=1.11e-06]\n",
            "Training:   1%|          | 1335/150000 [01:21<8:02:03,  5.14 iter/s, loss=43.3630, avg_loss=183.2895, lr=1.11e-06]\n",
            "Training:   1%|          | 1335/150000 [01:22<8:02:03,  5.14 iter/s, loss=60.5809, avg_loss=179.3561, lr=1.12e-06]\n",
            "Training:   1%|          | 1341/150000 [01:22<8:02:33,  5.13 iter/s, loss=60.5809, avg_loss=179.3561, lr=1.12e-06]\n",
            "Training:   1%|          | 1347/150000 [01:23<7:51:29,  5.25 iter/s, loss=60.5809, avg_loss=179.3561, lr=1.12e-06]\n",
            "Training:   1%|          | 1347/150000 [01:24<7:51:29,  5.25 iter/s, loss=50.1074, avg_loss=178.8672, lr=1.12e-06]\n",
            "Training:   1%|          | 1353/150000 [01:25<8:05:00,  5.11 iter/s, loss=50.1074, avg_loss=178.8672, lr=1.12e-06]\n",
            "Training:   1%|          | 1359/150000 [01:26<7:57:48,  5.18 iter/s, loss=50.1074, avg_loss=178.8672, lr=1.12e-06]\n",
            "Training:   1%|          | 1359/150000 [01:26<7:57:48,  5.18 iter/s, loss=17.6624, avg_loss=179.2278, lr=1.13e-06]\n",
            "Training:   1%|          | 1365/150000 [01:27<8:08:04,  5.08 iter/s, loss=17.6624, avg_loss=179.2278, lr=1.13e-06]\n",
            "Training:   1%|          | 1365/150000 [01:28<8:08:04,  5.08 iter/s, loss=33.2363, avg_loss=175.6114, lr=1.14e-06]\n",
            "Training:   1%|          | 1371/150000 [01:28<8:09:31,  5.06 iter/s, loss=33.2363, avg_loss=175.6114, lr=1.14e-06]\n",
            "Training:   1%|          | 1377/150000 [01:29<8:05:23,  5.10 iter/s, loss=33.2363, avg_loss=175.6114, lr=1.14e-06]\n",
            "Training:   1%|          | 1377/150000 [01:30<8:05:23,  5.10 iter/s, loss=41.3917, avg_loss=176.0326, lr=1.15e-06]\n",
            "Training:   1%|          | 1383/150000 [01:30<8:02:12,  5.14 iter/s, loss=41.3917, avg_loss=176.0326, lr=1.15e-06]\n",
            "Training:   1%|          | 1389/150000 [01:32<8:03:30,  5.12 iter/s, loss=41.3917, avg_loss=176.0326, lr=1.15e-06]\n",
            "Training:   1%|          | 1389/150000 [01:32<8:03:30,  5.12 iter/s, loss=9.4085, avg_loss=176.0616, lr=1.16e-06] \n",
            "Training:   1%|          | 1395/150000 [01:33<8:00:43,  5.15 iter/s, loss=9.4085, avg_loss=176.0616, lr=1.16e-06]\n",
            "Training:   1%|          | 1395/150000 [01:34<8:00:43,  5.15 iter/s, loss=27.8639, avg_loss=173.9085, lr=1.17e-06]\n",
            "Training:   1%|          | 1401/150000 [01:34<8:30:45,  4.85 iter/s, loss=27.8639, avg_loss=173.9085, lr=1.17e-06]\n",
            "Training:   1%|          | 1407/150000 [01:35<8:17:45,  4.98 iter/s, loss=27.8639, avg_loss=173.9085, lr=1.17e-06]\n",
            "Training:   1%|          | 1407/150000 [01:36<8:17:45,  4.98 iter/s, loss=65.0051, avg_loss=174.3009, lr=1.17e-06]\n",
            "Training:   1%|          | 1413/150000 [01:37<8:13:49,  5.01 iter/s, loss=65.0051, avg_loss=174.3009, lr=1.17e-06]\n",
            "Training:   1%|          | 1419/150000 [01:38<8:01:30,  5.14 iter/s, loss=65.0051, avg_loss=174.3009, lr=1.17e-06]\n",
            "Training:   1%|          | 1419/150000 [01:38<8:01:30,  5.14 iter/s, loss=44.2370, avg_loss=171.6019, lr=1.18e-06]\n",
            "Training:   1%|          | 1425/150000 [01:39<8:10:36,  5.05 iter/s, loss=44.2370, avg_loss=171.6019, lr=1.18e-06]\n",
            "Training:   1%|          | 1425/150000 [01:40<8:10:36,  5.05 iter/s, loss=40.7427, avg_loss=172.6844, lr=1.19e-06]\n",
            "Training:   1%|          | 1431/150000 [01:40<8:04:35,  5.11 iter/s, loss=40.7427, avg_loss=172.6844, lr=1.19e-06]\n",
            "Training:   1%|          | 1437/150000 [01:41<8:16:42,  4.98 iter/s, loss=40.7427, avg_loss=172.6844, lr=1.19e-06]\n",
            "Training:   1%|          | 1437/150000 [01:42<8:16:42,  4.98 iter/s, loss=70.6852, avg_loss=175.8157, lr=1.20e-06]\n",
            "Training:   1%|          | 1443/150000 [01:42<8:09:06,  5.06 iter/s, loss=70.6852, avg_loss=175.8157, lr=1.20e-06]\n",
            "Training:   1%|          | 1449/150000 [01:44<8:05:17,  5.10 iter/s, loss=70.6852, avg_loss=175.8157, lr=1.20e-06]\n",
            "Training:   1%|          | 1449/150000 [01:44<8:05:17,  5.10 iter/s, loss=11.3474, avg_loss=177.3380, lr=1.21e-06]\n",
            "Training:   1%|          | 1455/150000 [01:45<7:59:39,  5.16 iter/s, loss=11.3474, avg_loss=177.3380, lr=1.21e-06]\n",
            "Training:   1%|          | 1455/150000 [01:46<7:59:39,  5.16 iter/s, loss=73.2676, avg_loss=174.6847, lr=1.22e-06]\n",
            "Training:   1%|          | 1461/150000 [01:46<8:19:54,  4.95 iter/s, loss=73.2676, avg_loss=174.6847, lr=1.22e-06]\n",
            "Training:   1%|          | 1467/150000 [01:47<8:08:51,  5.06 iter/s, loss=73.2676, avg_loss=174.6847, lr=1.22e-06]\n",
            "Training:   1%|          | 1467/150000 [01:48<8:08:51,  5.06 iter/s, loss=23.5118, avg_loss=171.8598, lr=1.22e-06]\n",
            "Training:   1%|          | 1473/150000 [01:48<8:07:34,  5.08 iter/s, loss=23.5118, avg_loss=171.8598, lr=1.22e-06]\n",
            "Training:   1%|          | 1479/150000 [01:49<7:55:58,  5.20 iter/s, loss=23.5118, avg_loss=171.8598, lr=1.22e-06]\n",
            "Training:   1%|          | 1479/150000 [01:50<7:55:58,  5.20 iter/s, loss=21.8723, avg_loss=170.5647, lr=1.23e-06]\n",
            "Training:   1%|          | 1485/150000 [01:51<8:00:02,  5.16 iter/s, loss=21.8723, avg_loss=170.5647, lr=1.23e-06]\n",
            "Training:   1%|          | 1485/150000 [01:52<8:00:02,  5.16 iter/s, loss=47.5627, avg_loss=168.8563, lr=1.24e-06]\n"
          ]
        }
      ],
      "source": [
        "# 6. Executar Treinamento\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Configurar ambiente\n",
        "os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + \":\" + os.path.join(os.getcwd(), 'splade')\n",
        "os.environ['SPLADE_CONFIG_NAME'] = \"config_splade_pt.yaml\"\n",
        "os.environ['PYTHONUNBUFFERED'] = '1'  # Desabilita buffering para ver logs em tempo real\n",
        "\n",
        "print(\"üöÄ Iniciando treinamento... Acompanhe os logs abaixo.\")\n",
        "print(\"Nota: Ignorar avisos 'Unable to register cuFFT/cuDNN' do TensorFlow/JAX.\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Executar treinamento com output em tempo real usando subprocess\n",
        "cmd = [\n",
        "    sys.executable,  # Usa o Python do ambiente virtual\n",
        "    '-m', 'splade.train_from_triplets_ids',\n",
        "    'config.checkpoint_dir=experiments/pt/checkpoint',\n",
        "    'config.index_dir=experiments/pt/index',\n",
        "    'config.out_dir=experiments/pt/out'\n",
        "]\n",
        "\n",
        "# Mudar para o diret√≥rio splade\n",
        "original_dir = os.getcwd()\n",
        "os.chdir('splade')\n",
        "\n",
        "try:\n",
        "    # Executar com output em tempo real\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1,  # Line buffered\n",
        "        env=os.environ.copy()\n",
        "    )\n",
        "    \n",
        "    # Imprimir output em tempo real\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    \n",
        "    process.wait()\n",
        "    \n",
        "    if process.returncode != 0:\n",
        "        print(f\"\\n‚ùå Treinamento finalizado com c√≥digo de sa√≠da: {process.returncode}\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Treinamento conclu√≠do com sucesso!\")\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Treinamento interrompido pelo usu√°rio.\")\n",
        "    if 'process' in locals():\n",
        "        process.terminate()\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Erro durante o treinamento: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    os.chdir(original_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
