{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6sQDj08qzEY"
      },
      "source": [
        "# SPLADE v2 PT-BR - Treinamento Corrigido\n",
        "\n",
        "Este notebook cont√©m todas as corre√ß√µes necess√°rias para rodar o treinamento do SPLADE em 2025, resolvendo incompatibilidades de bibliotecas (AdamW/Hydra) e depend√™ncias de arquivos.\n",
        "\n",
        "**Aten√ß√£o:** Certifique-se de usar um Runtime com GPU (T4 ou A100)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuwHWS4pqzEZ",
        "outputId": "a30465dd-8f8d-4346-eceb-9de8bc8499bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrec_eval\n",
            "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytrec_eval\n",
            "  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp312-cp312-linux_x86_64.whl size=309354 sha256=f9393acd84cf0f3805d28ffb1547d96bd4282179fa9daf1ee4777ec1f62bfcf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/4a/9e/e17f9ea004e1c221bd0ff384732285211c4917b790d598ea51\n",
            "Successfully built pytrec_eval\n",
            "Installing collected packages: pytrec_eval\n",
            "Successfully installed pytrec_eval-0.5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for SPLADE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting hydra-core\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core) (25.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hydra-core\n",
            "Successfully installed hydra-core-1.3.2\n"
          ]
        }
      ],
      "source": [
        "# 1. Instala√ß√£o de Bibliotecas\n",
        "!pip install pytrec_eval\n",
        "!pip install git+https://github.com/leobavila/splade.git -q\n",
        "!pip install hydra-core --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZNtUoOCqzEa",
        "outputId": "eda3c9f6-239e-4031-adc5-65f0758f70f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'splade'...\n",
            "remote: Enumerating objects: 495, done.\u001b[K\n",
            "remote: Counting objects: 100% (244/244), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 495 (delta 170), reused 136 (delta 136), pack-reused 251 (from 1)\u001b[K\n",
            "Receiving objects: 100% (495/495), 3.08 MiB | 4.71 MiB/s, done.\n",
            "Resolving deltas: 100% (238/238), done.\n",
            "‚úÖ Patch aplicado: bert_optim.py corrigido.\n"
          ]
        }
      ],
      "source": [
        "# 2. Clonagem e Patch de Corre√ß√£o (AdamW)\n",
        "import os\n",
        "\n",
        "# Clona o reposit√≥rio se n√£o existir\n",
        "if not os.path.exists(\"splade\"):\n",
        "    !git clone https://github.com/leobavila/splade.git\n",
        "\n",
        "# Corrige o erro de importa√ß√£o do AdamW (Transformers antigo vs novo)\n",
        "file_path = \"splade/splade/optim/bert_optim.py\"\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    new_content = content.replace(\n",
        "        \"from transformers.optimization import AdamW, get_linear_schedule_with_warmup\",\n",
        "        \"from transformers import get_linear_schedule_with_warmup; from torch.optim import AdamW\"\n",
        "    )\n",
        "\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(new_content)\n",
        "    print(\"‚úÖ Patch aplicado: bert_optim.py corrigido.\")\n",
        "else:\n",
        "    print(\"‚ùå Erro: Arquivo bert_optim.py n√£o encontrado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHizHlnbqzEa",
        "outputId": "f934648f-4b7b-4f18-addd-73507308ac65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Baixando datasets p√∫blicos... (Pode levar 2-3 minutos)\n",
            "‚úÖ Datasets baixados e organizados.\n"
          ]
        }
      ],
      "source": [
        "# 3. Download e Prepara√ß√£o dos Datasets (mMARCO e mRobust)\n",
        "import shutil\n",
        "\n",
        "print(\"‚è≥ Baixando datasets p√∫blicos... (Pode levar 2-3 minutos)\")\n",
        "\n",
        "# Criar pastas base\n",
        "os.makedirs(\"/content/data/m_marco\", exist_ok=True)\n",
        "os.makedirs(\"/content/data/m_robust\", exist_ok=True)\n",
        "\n",
        "\n",
        "# Criar pastas de destino do SPLADE\n",
        "os.makedirs(\"splade/data/pt/triplets\", exist_ok=True)\n",
        "os.makedirs(\"splade/data/pt/val_retrieval/collection\", exist_ok=True)\n",
        "os.makedirs(\"splade/data/pt/val_retrieval/queries\", exist_ok=True)\n",
        "\n",
        "# --- mMARCO (Treino) ---\n",
        "!wget -q https://huggingface.co/datasets/unicamp-dl/mmarco/resolve/main/data/google/queries/train/portuguese.tsv -O /content/data/m_marco/queries_train.tsv\n",
        "!wget -q https://huggingface.co/datasets/unicamp-dl/mmarco/resolve/main/data/google/collections/portuguese.tsv -O /content/data/m_marco/corpus.tsv\n",
        "!wget -q https://huggingface.co/datasets/unicamp-dl/mmarco/resolve/main/data/triples.train.ids.small.tsv -O /content/data/m_marco/triples.train.ids.small.tsv\n",
        "\n",
        "# Copiar para estrutura SPLADE\n",
        "shutil.copy(\"/content/data/m_marco/corpus.tsv\", \"splade/data/pt/triplets/corpus.tsv\")\n",
        "shutil.copy(\"/content/data/m_marco/queries_train.tsv\", \"splade/data/pt/triplets/queries_train.tsv\")\n",
        "shutil.copy(\"/content/data/m_marco/triples.train.ids.small.tsv\", \"splade/data/pt/triplets/raw.tsv\")\n",
        "\n",
        "# --- mRobust (Valida√ß√£o) ---\n",
        "!wget -q https://huggingface.co/datasets/unicamp-dl/mrobust/resolve/main/data/mrobust/queries.tsv -O /content/data/m_robust/queries.tsv\n",
        "!wget -q https://huggingface.co/datasets/unicamp-dl/mrobust/resolve/main/data/mrobust/corpus.tsv -O /content/data/m_robust/corpus.tsv\n",
        "!wget -q https://huggingface.co/datasets/unicamp-dl/mrobust/resolve/main/data/mrobust/qrels.robust04.txt -O /content/data/m_robust/qrels.robust04.txt\n",
        "\n",
        "# Copiar para estrutura SPLADE (Valida√ß√£o)\n",
        "shutil.copy(\"/content/data/m_robust/corpus.tsv\", \"splade/data/pt/val_retrieval/collection/raw.tsv\")\n",
        "shutil.copy(\"/content/data/m_robust/queries.tsv\", \"splade/data/pt/val_retrieval/queries/raw.tsv\")\n",
        "\n",
        "print(\"‚úÖ Datasets baixados e organizados.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32EwFsPvqzEa",
        "outputId": "ff444073-7f81-4edd-e47b-3b06a87d9dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ QREL convertido para JSON.\n"
          ]
        }
      ],
      "source": [
        "# 4. Converter QRELS para JSON\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "qrel = defaultdict(dict)\n",
        "qrel_path = \"/content/data/m_robust/qrels.robust04.txt\"\n",
        "\n",
        "if os.path.exists(qrel_path):\n",
        "    with open(qrel_path, 'r') as file:\n",
        "        for line in file:\n",
        "            fields = line.split()\n",
        "            if len(fields) >= 4:\n",
        "                q_id = fields[0]\n",
        "                doc_id = fields[2]\n",
        "                rel = fields[3]\n",
        "                qrel[q_id][doc_id] = int(rel)\n",
        "\n",
        "    with open('splade/data/pt/val_retrieval/qrel.json', 'w') as file:\n",
        "        json.dump(qrel, file)\n",
        "    print(\"‚úÖ QREL convertido para JSON.\")\n",
        "else:\n",
        "    print(\"‚ùå Erro: qrels.robust04.txt n√£o encontrado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7m9IhRyqzEa",
        "outputId": "2b201a33-c26f-4b39-a316-d650d9173b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Configura√ß√µes recriadas com sucesso (loss: InBatchPairwiseNLL adicionado).\n"
          ]
        }
      ],
      "source": [
        "# 5. Gerar Arquivos de Configura√ß√£o (CR√çTICO: Inclus√£o do par√¢metro 'loss')\n",
        "# Corre√ß√£o: Adicionado 'loss: InBatchPairwiseNLL' para corrigir o ConfigKeyError.\n",
        "\n",
        "import os\n",
        "\n",
        "# Criar estrutura de pastas\n",
        "os.makedirs(\"splade/conf/train/config\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/train/data\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/train/model\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/index\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/retrieve_evaluate\", exist_ok=True)\n",
        "os.makedirs(\"splade/conf/flops\", exist_ok=True)\n",
        "\n",
        "# 5.1 Modelo\n",
        "with open(\"splade/conf/train/model/splade_bertimbau_base.yaml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "_target_: splade.models.transformer_rep.Splade\n",
        "# Nota: O par√¢metro real ser√° lido do init_dict abaixo\n",
        "model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
        "    \"\"\")\n",
        "\n",
        "# 5.2 Dados\n",
        "with open(\"splade/conf/train/data/pt.yaml\", \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "# @package _global_\n",
        "data:\n",
        "    type: triplets\n",
        "    TRAIN_DATA_DIR: {os.getcwd()}/splade/data/pt/triplets\n",
        "    VALIDATION_DATA_DIR: {os.getcwd()}/splade/data/pt/val_retrieval\n",
        "    QREL_PATH: {os.getcwd()}/splade/data/pt/val_retrieval/qrel.json\n",
        "    \"\"\")\n",
        "\n",
        "# 5.3 Config de Treino (CORRE√á√ÉO AQUI: Adicionado 'loss')\n",
        "with open(\"splade/conf/train/config/splade_pt.yaml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "# @package _global_\n",
        "config:\n",
        "    lr: 2e-5\n",
        "    seed: 123\n",
        "    gradient_accumulation_steps: 1\n",
        "    weight_decay: 0.01\n",
        "    validation_metrics: [MRR@10]\n",
        "    pretrained_no_yaml_config: false\n",
        "    nb_iterations: 150000\n",
        "    train_batch_size: 32\n",
        "    eval_batch_size: 32\n",
        "    index_retrieval_batch_size: 32\n",
        "    record_frequency: 1000\n",
        "    train_monitoring_freq: 500\n",
        "    warmup_steps: 6000\n",
        "    max_length: 256\n",
        "    fp16: true\n",
        "    matching_type: splade\n",
        "    monitoring_ckpt: true\n",
        "    tokenizer_type: neuralmind/bert-base-portuguese-cased\n",
        "\n",
        "    # Par√¢metro de perda que faltava:\n",
        "    loss: InBatchPairwiseNLL\n",
        "\n",
        "    # Chaves obrigat√≥rias para o Hydra:\n",
        "    checkpoint_dir: \"\"\n",
        "    index_dir: \"\"\n",
        "    out_dir: \"\"\n",
        "\n",
        "    regularization:\n",
        "        FLOPS:\n",
        "            lambda_q: 0.0003\n",
        "            lambda_d: 0.0001\n",
        "            T: 50000\n",
        "    \"\"\")\n",
        "\n",
        "# 5.4 Config Geral\n",
        "with open(\"splade/conf/config_splade_pt.yaml\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "defaults:\n",
        "  - train/data: pt\n",
        "  - train/model: splade_bertimbau_base\n",
        "  - train/config: splade_pt\n",
        "  - index: pt\n",
        "  - retrieve_evaluate: pt\n",
        "  - flops: pt\n",
        "  - _self_\n",
        "\n",
        "# init_dict com as corre√ß√µes anteriores\n",
        "init_dict:\n",
        "  model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
        "  fp16: true\n",
        "\n",
        "hydra:\n",
        "  run:\n",
        "    dir: experiments/pt/out\n",
        "  job:\n",
        "    chdir: true\n",
        "    \"\"\")\n",
        "\n",
        "# 5.5 Placeholders\n",
        "with open(\"splade/conf/index/pt.yaml\", \"w\") as f: f.write(\"# Placeholder\")\n",
        "with open(\"splade/conf/retrieve_evaluate/pt.yaml\", \"w\") as f: f.write(\"# Placeholder\")\n",
        "with open(\"splade/conf/flops/pt.yaml\", \"w\") as f: f.write(\"# Placeholder\")\n",
        "\n",
        "print(\"‚úÖ Configura√ß√µes recriadas com sucesso (loss: InBatchPairwiseNLL adicionado).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGCzclpCqzEa",
        "outputId": "4e9da717-5ada-4375-d9b9-b0c7e45c4267",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Iniciando treinamento... Acompanhe os logs abaixo.\n",
            "Nota: Ignorar avisos 'Unable to register cuFFT/cuDNN' do TensorFlow/JAX.\n",
            "2025-11-28 11:44:36.047415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764330276.078778    1203 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764330276.088425    1203 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764330276.111848    1203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764330276.111881    1203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764330276.111887    1203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764330276.111890    1203 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 11:44:36.116585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/splade/splade/train_from_triplets_ids.py:20: UserWarning: \n",
            "The version_base parameter is not specified.\n",
            "Please specify a compatability version level, or None.\n",
            "Will assume defaults for version 1.1\n",
            "  @hydra.main(config_path=CONFIG_PATH, config_name=CONFIG_NAME)\n",
            "data:\n",
            "  type: triplets\n",
            "  TRAIN_DATA_DIR: /content/splade/data/pt/triplets\n",
            "  VALIDATION_DATA_DIR: /content/splade/data/pt/val_retrieval\n",
            "  QREL_PATH: /content/splade/data/pt/val_retrieval/qrel.json\n",
            "train:\n",
            "  model:\n",
            "    _target_: splade.models.transformer_rep.Splade\n",
            "    model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
            "config:\n",
            "  lr: 2.0e-05\n",
            "  seed: 123\n",
            "  gradient_accumulation_steps: 1\n",
            "  weight_decay: 0.01\n",
            "  validation_metrics:\n",
            "  - MRR@10\n",
            "  pretrained_no_yaml_config: false\n",
            "  nb_iterations: 150000\n",
            "  train_batch_size: 32\n",
            "  eval_batch_size: 32\n",
            "  index_retrieval_batch_size: 32\n",
            "  record_frequency: 1000\n",
            "  train_monitoring_freq: 500\n",
            "  warmup_steps: 6000\n",
            "  max_length: 256\n",
            "  fp16: true\n",
            "  matching_type: splade\n",
            "  monitoring_ckpt: true\n",
            "  tokenizer_type: neuralmind/bert-base-portuguese-cased\n",
            "  loss: InBatchPairwiseNLL\n",
            "  checkpoint_dir: experiments/pt/checkpoint\n",
            "  index_dir: experiments/pt/index\n",
            "  out_dir: experiments/pt/out\n",
            "  regularization:\n",
            "    FLOPS:\n",
            "      lambda_q: 0.0003\n",
            "      lambda_d: 0.0001\n",
            "      T: 50000\n",
            "index: {}\n",
            "retrieve_evaluate: {}\n",
            "flops: {}\n",
            "init_dict:\n",
            "  model_type_or_dir: neuralmind/bert-base-portuguese-cased\n",
            "  fp16: true\n",
            "\n",
            "config.json: 100% 647/647 [00:00<00:00, 4.43MB/s]\n",
            "pytorch_model.bin: 100% 438M/438M [00:05<00:00, 73.7MB/s]\n",
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tokenizer_config.json: 100% 43.0/43.0 [00:00<00:00, 291kB/s]\n",
            "vocab.txt: 210kB [00:00, 76.1MB/s]\n",
            "model.safetensors:   0% 0.00/438M [00:00<?, ?B/s]\n",
            "added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 13.6kB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.21MB/s]\n",
            "Preloading dataset\n",
            "\n",
            "model.safetensors:   1% 2.30M/438M [00:01<03:49, 1.90MB/s]\n",
            "model.safetensors:   3% 13.4M/438M [00:01<00:31, 13.3MB/s]\n",
            "model.safetensors:   6% 26.9M/438M [00:01<00:15, 26.5MB/s]\n",
            "model.safetensors:   8% 37.2M/438M [00:01<00:10, 36.6MB/s]\n",
            "model.safetensors:  13% 55.8M/438M [00:01<00:06, 60.9MB/s]\n",
            "model.safetensors:  18% 80.2M/438M [00:01<00:03, 95.0MB/s]\n",
            "model.safetensors:  23% 103M/438M [00:02<00:03, 86.6MB/s] \n",
            "model.safetensors:  35% 154M/438M [00:02<00:01, 156MB/s] \n",
            "model.safetensors:  40% 177M/438M [00:02<00:01, 168MB/s]\n",
            "458889it [00:01, 332418.26it/s]\u001b[A\n",
            "496543it [00:01, 333982.25it/s]\u001b[A\n",
            "model.safetensors:  56% 247M/438M [00:02<00:01, 187MB/s]\n",
            "model.safetensors:  67% 294M/438M [00:02<00:00, 232MB/s]\n",
            "model.safetensors:  78% 340M/438M [00:02<00:00, 271MB/s]\n",
            "681160it [00:01, 351300.08it/s]\u001b[A\n",
            "721216it [00:01, 364064.61it/s]\u001b[A\n",
            "785951it [00:02, 440608.66it/s]\u001b[A\n",
            "858669it [00:02, 520384.61it/s]\u001b[A\n",
            "model.safetensors: 100% 438M/438M [00:03<00:00, 126MB/s]\n",
            "\n",
            "994176it [00:02, 585262.74it/s]\u001b[A\n",
            "1092999it [00:02, 702198.99it/s]\u001b[A\n",
            "1217801it [00:02, 862157.54it/s]\u001b[A\n",
            "1348075it [00:02, 992164.60it/s]\u001b[A\n",
            "1448370it [00:02, 905812.88it/s]\u001b[A\n",
            "1587408it [00:02, 1040634.94it/s]\u001b[A\n",
            "1729459it [00:02, 1148675.99it/s]\u001b[A\n",
            "1869354it [00:03, 1220967.44it/s]\u001b[A\n",
            "2002624it [00:03, 1253595.47it/s]\u001b[A\n",
            "2148225it [00:03, 1313189.59it/s]\u001b[A\n",
            "2292753it [00:03, 1352290.20it/s]\u001b[A\n",
            "2428827it [00:03, 1308669.15it/s]\u001b[A\n",
            "2573573it [00:03, 1349027.90it/s]\u001b[A\n",
            "2715680it [00:03, 1370072.10it/s]\u001b[A\n",
            "2853253it [00:03, 1037602.15it/s]\u001b[A\n",
            "2999087it [00:03, 1139412.31it/s]\u001b[A\n",
            "3148215it [00:04, 1229794.45it/s]\u001b[A\n",
            "3293759it [00:04, 1290356.36it/s]\u001b[A\n",
            "3435048it [00:04, 1324313.11it/s]\u001b[A\n",
            "3582750it [00:04, 1367620.67it/s]\u001b[A\n",
            "3723477it [00:04, 1315063.59it/s]\u001b[A\n",
            "3865735it [00:04, 1345376.11it/s]\u001b[A\n",
            "4012192it [00:04, 1379598.85it/s]\u001b[A\n",
            "4156201it [00:04, 1397183.99it/s]\u001b[A\n",
            "4302989it [00:04, 1417928.87it/s]\u001b[A\n",
            "4447267it [00:04, 1425262.25it/s]\u001b[A\n",
            "4592938it [00:05, 1434570.59it/s]\u001b[A\n",
            "4736891it [00:05, 1424714.56it/s]\u001b[A\n",
            "4879720it [00:05, 1267728.81it/s]\u001b[A\n",
            "5009874it [00:05, 1081868.09it/s]\u001b[A\n",
            "5124521it [00:05, 991332.00it/s] \u001b[A\n",
            "5228790it [00:05, 949542.66it/s]\u001b[A\n",
            "5327103it [00:05, 917251.86it/s]\u001b[A\n",
            "5420938it [00:06, 888050.80it/s]\u001b[A\n",
            "5511019it [00:06, 871416.69it/s]\u001b[A\n",
            "5598899it [00:06, 491986.90it/s]\u001b[A\n",
            "5669482it [00:06, 529548.80it/s]\u001b[A\n",
            "5757654it [00:06, 600193.28it/s]\u001b[A\n",
            "5837850it [00:06, 645130.80it/s]\u001b[A\n",
            "5922554it [00:06, 693859.94it/s]\u001b[A\n",
            "6005960it [00:07, 729803.67it/s]\u001b[A\n",
            "6089076it [00:07, 756970.79it/s]\u001b[A\n",
            "6171747it [00:07, 776276.39it/s]\u001b[A\n",
            "6255117it [00:07, 792544.29it/s]\u001b[A\n",
            "6338144it [00:07, 803411.09it/s]\u001b[A\n",
            "6420524it [00:07, 802997.41it/s]\u001b[A\n",
            "6502251it [00:07, 793711.56it/s]\u001b[A\n",
            "6612861it [00:07, 884674.60it/s]\u001b[A\n",
            "6744063it [00:07, 1010215.02it/s]\u001b[A\n",
            "6886390it [00:07, 1132315.25it/s]\u001b[A\n",
            "7028627it [00:08, 1218429.90it/s]\u001b[A\n",
            "7172545it [00:08, 1284166.08it/s]\u001b[A\n",
            "7311433it [00:08, 1315407.54it/s]\u001b[A\n",
            "7451193it [00:08, 1339961.73it/s]\u001b[A\n",
            "7594666it [00:08, 1368310.77it/s]\u001b[A\n",
            "7735961it [00:08, 1381668.55it/s]\u001b[A\n",
            "7881385it [00:08, 1403402.45it/s]\u001b[A\n",
            "8021829it [00:08, 1330461.96it/s]\u001b[A\n",
            "8169391it [00:08, 1372357.35it/s]\u001b[A\n",
            "8313449it [00:08, 1392266.41it/s]\u001b[A\n",
            "8459660it [00:09, 1412814.07it/s]\u001b[A\n",
            "8605234it [00:09, 1425510.25it/s]\u001b[A\n",
            "8750389it [00:09, 1433240.53it/s]\u001b[A\n",
            "8894733it [00:09, 1436264.39it/s]\u001b[A\n",
            "9041283it [00:09, 1444981.71it/s]\u001b[A\n",
            "9185905it [00:09, 1424088.12it/s]\u001b[A\n",
            "9331908it [00:09, 1434721.08it/s]\u001b[A\n",
            "9475499it [00:09, 1380199.55it/s]\u001b[A\n",
            "9614009it [00:09, 1376108.67it/s]\u001b[A\n",
            "9761381it [00:09, 1404613.62it/s]\u001b[A\n",
            "9902153it [00:10, 1401003.47it/s]\u001b[A\n",
            "10047844it [00:10, 1417508.76it/s]\u001b[A\n",
            "10189770it [00:10, 1416742.75it/s]\u001b[A\n",
            "10336065it [00:10, 1430477.78it/s]\u001b[A\n",
            "10479210it [00:10, 1426892.92it/s]\u001b[A\n",
            "10621967it [00:10, 1416953.91it/s]\u001b[A\n",
            "10763721it [00:10, 1415795.00it/s]\u001b[A\n",
            "10905341it [00:10, 1356989.07it/s]\u001b[A\n",
            "11049092it [00:10, 1380305.99it/s]\u001b[A\n",
            "11187567it [00:11, 590670.93it/s] \u001b[A\n",
            "11326994it [00:11, 712540.31it/s]\u001b[A\n",
            "11467215it [00:11, 835348.01it/s]\u001b[A\n",
            "11600851it [00:11, 936949.45it/s]\u001b[A\n",
            "11726686it [00:11, 1008503.63it/s]\u001b[A\n",
            "11869974it [00:11, 1111540.57it/s]\u001b[A\n",
            "12016144it [00:12, 1201715.21it/s]\u001b[A\n",
            "12159682it [00:12, 1264556.80it/s]\u001b[A\n",
            "12303573it [00:12, 1312926.14it/s]\u001b[A\n",
            "12447241it [00:12, 1348054.37it/s]\u001b[A\n",
            "12588147it [00:12, 1362041.21it/s]\u001b[A\n",
            "12728668it [00:12, 1370893.89it/s]\u001b[A\n",
            "12868800it [00:12, 1379347.66it/s]\u001b[A\n",
            "13008883it [00:12, 1374466.67it/s]\u001b[A\n",
            "13147835it [00:12, 1312415.14it/s]\u001b[A\n",
            "13289754it [00:12, 1342969.36it/s]\u001b[A\n",
            "13432113it [00:13, 1366360.31it/s]\u001b[A\n",
            "13569715it [00:13, 1363764.61it/s]\u001b[A\n",
            "13708855it [00:13, 1371888.71it/s]\u001b[A\n",
            "13848127it [00:13, 1378045.27it/s]\u001b[A\n",
            "13987264it [00:13, 1381994.76it/s]\u001b[A\n",
            "14126478it [00:13, 1385002.89it/s]\u001b[A\n",
            "14265457it [00:13, 1386421.90it/s]\u001b[A\n",
            "14405278it [00:13, 1389934.33it/s]\u001b[A\n",
            "14544359it [00:13, 1315308.23it/s]\u001b[A\n",
            "14681043it [00:13, 1330081.97it/s]\u001b[A\n",
            "14822196it [00:14, 1353813.04it/s]\u001b[A\n",
            "14959743it [00:14, 1360173.03it/s]\u001b[A\n",
            "15100501it [00:14, 1374185.60it/s]\u001b[A\n",
            "15239781it [00:14, 1379702.32it/s]\u001b[A\n",
            "15378972it [00:14, 1383329.71it/s]\u001b[A\n",
            "15521334it [00:14, 1395340.27it/s]\u001b[A\n",
            "15660980it [00:14, 1371369.59it/s]\u001b[A\n",
            "15806253it [00:14, 1395403.97it/s]\u001b[A\n",
            "15945945it [00:14, 1384901.01it/s]\u001b[A\n",
            "16084549it [00:14, 1342298.97it/s]\u001b[A\n",
            "16227195it [00:15, 1366762.45it/s]\u001b[A\n",
            "16364174it [00:15, 1350982.58it/s]\u001b[A\n",
            "16505145it [00:15, 1368172.60it/s]\u001b[A\n",
            "16643975it [00:15, 1374099.29it/s]\u001b[A\n",
            "16781534it [00:15, 1368975.93it/s]\u001b[A\n",
            "16924186it [00:15, 1386036.84it/s]\u001b[A\n",
            "17062906it [00:15, 1386367.71it/s]\u001b[A\n",
            "17201607it [00:15, 1372310.66it/s]\u001b[A\n",
            "17342163it [00:15, 1382160.00it/s]\u001b[A\n",
            "17480440it [00:16, 1331085.06it/s]\u001b[A\n",
            "17619814it [00:16, 1349275.53it/s]\u001b[A\n",
            "17759288it [00:16, 1362599.97it/s]\u001b[A\n",
            "17899799it [00:16, 1375142.77it/s]\u001b[A\n",
            "18038825it [00:16, 1379541.21it/s]\u001b[A\n",
            "18177333it [00:16, 1381174.55it/s]\u001b[A\n",
            "18316323it [00:16, 1383761.45it/s]\u001b[A\n",
            "18454777it [00:16, 1374609.92it/s]\u001b[A\n",
            "18593447it [00:16, 1378202.46it/s]\u001b[A\n",
            "18733238it [00:16, 1384069.39it/s]\u001b[A\n",
            "18871682it [00:17, 1320733.21it/s]\u001b[A\n",
            "19015100it [00:17, 1353564.98it/s]\u001b[A\n",
            "19151010it [00:17, 1347754.33it/s]\u001b[A\n",
            "19286166it [00:17, 1335380.22it/s]\u001b[A\n",
            "19427949it [00:17, 1359562.64it/s]\u001b[A\n",
            "19565716it [00:17, 1364896.70it/s]\u001b[A\n",
            "19702384it [00:17, 1351556.93it/s]\u001b[A\n",
            "19837682it [00:17, 1175648.41it/s]\u001b[A\n",
            "19959317it [00:17, 1042975.70it/s]\u001b[A\n",
            "20068585it [00:18, 950713.65it/s] \u001b[A\n",
            "20167883it [00:18, 902797.80it/s]\u001b[A\n",
            "20260969it [00:18, 866908.52it/s]\u001b[A\n",
            "20349407it [00:18, 828565.91it/s]\u001b[A\n",
            "20433336it [00:18, 808682.68it/s]\u001b[A\n",
            "20514777it [00:18, 809845.99it/s]\u001b[A\n",
            "20599670it [00:18, 820470.31it/s]\u001b[A\n",
            "20687631it [00:18, 834998.85it/s]\u001b[A\n",
            "20773943it [00:18, 843009.04it/s]\u001b[A\n",
            "20862959it [00:19, 856647.07it/s]\u001b[A\n",
            "20948888it [00:19, 796470.61it/s]\u001b[A\n",
            "21030507it [00:19, 801975.21it/s]\u001b[A\n",
            "21111400it [00:19, 803634.09it/s]\u001b[A\n",
            "21193050it [00:19, 806840.20it/s]\u001b[A\n",
            "21274092it [00:19, 805768.55it/s]\u001b[A\n",
            "21354917it [00:19, 802367.77it/s]\u001b[A\n",
            "21436440it [00:19, 806153.21it/s]\u001b[A\n",
            "21517182it [00:19, 804508.64it/s]\u001b[A\n",
            "21597721it [00:19, 800642.76it/s]\u001b[A\n",
            "21701444it [00:20, 870842.38it/s]\u001b[A\n",
            "21821385it [00:20, 968655.24it/s]\u001b[A\n",
            "21950790it [00:20, 1065741.74it/s]\u001b[A\n",
            "22085017it [00:20, 1148374.87it/s]\u001b[A\n",
            "22218306it [00:20, 1203574.42it/s]\u001b[A\n",
            "22348120it [00:20, 1231884.00it/s]\u001b[A\n",
            "22471395it [00:21, 326673.37it/s] \u001b[A\n",
            "22609710it [00:21, 434340.95it/s]\u001b[A\n",
            "22748032it [00:21, 554929.60it/s]\u001b[A\n",
            "22874296it [00:21, 663663.96it/s]\u001b[A\n",
            "23018615it [00:22, 803971.24it/s]\u001b[A\n",
            "23164691it [00:22, 938305.29it/s]\u001b[A\n",
            "23305092it [00:22, 1043605.79it/s]\u001b[A\n",
            "23439606it [00:22, 1101617.74it/s]\u001b[A\n",
            "23584499it [00:22, 1190491.64it/s]\u001b[A\n",
            "23730225it [00:22, 1261895.15it/s]\u001b[A\n",
            "23870522it [00:22, 1300913.53it/s]\u001b[A\n",
            "24018862it [00:22, 1352595.55it/s]\u001b[A\n",
            "24164687it [00:22, 1383023.35it/s]\u001b[A\n",
            "24312373it [00:22, 1410371.40it/s]\u001b[A\n",
            "24461569it [00:23, 1434327.58it/s]\u001b[A\n",
            "24608145it [00:23, 1443615.37it/s]\u001b[A\n",
            "24754268it [00:23, 1445584.29it/s]\u001b[A\n",
            "24900061it [00:23, 1373974.97it/s]\u001b[A\n",
            "25039051it [00:23, 1367865.52it/s]\u001b[A\n",
            "25176937it [00:23, 1364137.52it/s]\u001b[A\n",
            "25316594it [00:23, 1373577.74it/s]\u001b[A\n",
            "25454512it [00:23, 1363556.96it/s]\u001b[A\n",
            "25592200it [00:23, 1367461.11it/s]\u001b[A\n",
            "25731847it [00:23, 1376026.79it/s]\u001b[A\n",
            "25869660it [00:24, 1372237.49it/s]\u001b[A\n",
            "26009200it [00:24, 1379109.56it/s]\u001b[A\n",
            "26147608it [00:24, 1380581.57it/s]\u001b[A\n",
            "26285744it [00:24, 1304414.56it/s]\u001b[A\n",
            "26419803it [00:24, 1314790.87it/s]\u001b[A\n",
            "26551958it [00:24, 1305309.85it/s]\u001b[A\n",
            "26683452it [00:24, 1308112.12it/s]\u001b[A\n",
            "26816032it [00:24, 1313256.62it/s]\u001b[A\n",
            "26947601it [00:24, 1312886.85it/s]\u001b[A\n",
            "27081725it [00:24, 1321292.84it/s]\u001b[A\n",
            "27215566it [00:25, 1326380.45it/s]\u001b[A\n",
            "27348297it [00:25, 1294783.03it/s]\u001b[A\n",
            "27477995it [00:25, 1284647.73it/s]\u001b[A\n",
            "27606613it [00:25, 1263933.19it/s]\u001b[A\n",
            "27733330it [00:25, 1264869.94it/s]\u001b[A\n",
            "27866181it [00:25, 1283606.22it/s]\u001b[A\n",
            "27994649it [00:25, 1273501.40it/s]\u001b[A\n",
            "28122081it [00:25, 1264134.02it/s]\u001b[A\n",
            "28250262it [00:25, 1269345.76it/s]\u001b[A\n",
            "28382613it [00:25, 1285407.58it/s]\u001b[A\n",
            "28519999it [00:26, 1311730.29it/s]\u001b[A\n",
            "28651230it [00:26, 1304924.57it/s]\u001b[A\n",
            "28781768it [00:26, 1303876.70it/s]\u001b[A\n",
            "28920193it [00:26, 1327834.52it/s]\u001b[A\n",
            "29053013it [00:26, 1240186.12it/s]\u001b[A\n",
            "29188015it [00:26, 1271476.42it/s]\u001b[A\n",
            "29322511it [00:26, 1292728.28it/s]\u001b[A\n",
            "29452551it [00:26, 1272601.67it/s]\u001b[A\n",
            "29586685it [00:26, 1292598.75it/s]\u001b[A\n",
            "29716404it [00:27, 1141839.84it/s]\u001b[A\n",
            "29833980it [00:27, 1118818.19it/s]\u001b[A\n",
            "29967445it [00:27, 1177682.02it/s]\u001b[A\n",
            "30097897it [00:27, 1213241.86it/s]\u001b[A\n",
            "30221875it [00:27, 1215352.06it/s]\u001b[A\n",
            "30350340it [00:27, 1235382.34it/s]\u001b[A\n",
            "30481738it [00:27, 1258348.64it/s]\u001b[A\n",
            "30618573it [00:27, 1290742.30it/s]\u001b[A\n",
            "30756662it [00:27, 1317432.28it/s]\u001b[A\n",
            "30893130it [00:27, 1331467.16it/s]\u001b[A\n",
            "31031819it [00:28, 1347979.51it/s]\u001b[A\n",
            "31170686it [00:28, 1360119.69it/s]\u001b[A\n",
            "31306866it [00:28, 1352246.53it/s]\u001b[A\n",
            "31446550it [00:28, 1365525.65it/s]\u001b[A\n",
            "31584969it [00:28, 1371085.22it/s]\u001b[A\n",
            "31722148it [00:28, 1287654.88it/s]\u001b[A\n",
            "31851983it [00:28, 1249452.45it/s]\u001b[A\n",
            "31983724it [00:28, 1268717.09it/s]\u001b[A\n",
            "32111310it [00:28, 1244367.92it/s]\u001b[A\n",
            "32243969it [00:29, 1267992.35it/s]\u001b[A\n",
            "32382309it [00:29, 1301574.38it/s]\u001b[A\n",
            "32516194it [00:29, 1312512.54it/s]\u001b[A\n",
            "32648785it [00:29, 1316463.41it/s]\u001b[A\n",
            "32786807it [00:29, 1335380.36it/s]\u001b[A\n",
            "32923291it [00:29, 1344137.77it/s]\u001b[A\n",
            "33057840it [00:29, 1218958.06it/s]\u001b[A\n",
            "33194079it [00:29, 1258933.90it/s]\u001b[A\n",
            "33325495it [00:29, 1274655.33it/s]\u001b[A\n",
            "33459635it [00:29, 1293946.56it/s]\u001b[A\n",
            "33590097it [00:30, 1210503.81it/s]\u001b[A\n",
            "33712867it [00:30, 1103413.70it/s]\u001b[A\n",
            "33825856it [00:30, 982285.48it/s] \u001b[A\n",
            "33927709it [00:30, 971681.14it/s]\u001b[A\n",
            "34027283it [00:30, 790129.18it/s]\u001b[A\n",
            "34112615it [00:30, 794868.22it/s]\u001b[A\n",
            "34196642it [00:30, 791101.73it/s]\u001b[A\n",
            "34282693it [00:30, 808901.58it/s]\u001b[A\n",
            "34372589it [00:31, 833050.99it/s]\u001b[A\n",
            "34462872it [00:31, 852374.61it/s]\u001b[A\n",
            "34549688it [00:31, 854265.19it/s]\u001b[A\n",
            "34636229it [00:31, 842295.01it/s]\u001b[A\n",
            "34721240it [00:31, 841167.56it/s]\u001b[A\n",
            "34805900it [00:31, 814440.31it/s]\u001b[A\n",
            "34887845it [00:31, 735303.74it/s]\u001b[A\n",
            "34978533it [00:31, 781491.70it/s]\u001b[A\n",
            "35068219it [00:31, 813530.67it/s]\u001b[A\n",
            "35159563it [00:32, 841904.72it/s]\u001b[A\n",
            "35248690it [00:32, 856147.01it/s]\u001b[A\n",
            "35335165it [00:32, 831818.81it/s]\u001b[A\n",
            "35419058it [00:32, 812893.05it/s]\u001b[A\n",
            "35508653it [00:32, 836647.04it/s]\u001b[A\n",
            "35592806it [00:32, 822208.52it/s]\u001b[A\n",
            "35675386it [00:32, 795748.06it/s]\u001b[A\n",
            "35755321it [00:32, 776781.13it/s]\u001b[A\n",
            "35834453it [00:32, 780906.05it/s]\u001b[A\n",
            "35913539it [00:32, 783774.14it/s]\u001b[A\n",
            "35996673it [00:33, 797661.62it/s]\u001b[A\n",
            "36087223it [00:33, 829406.18it/s]\u001b[A\n",
            "36177822it [00:33, 852073.26it/s]\u001b[A\n",
            "36264756it [00:33, 857197.47it/s]\u001b[A\n",
            "36355292it [00:33, 871544.61it/s]\u001b[A\n",
            "36442531it [00:33, 840781.88it/s]\u001b[A\n",
            "36526893it [00:33, 808372.72it/s]\u001b[A\n",
            "36608126it [00:33, 804629.82it/s]\u001b[A\n",
            "36699015it [00:33, 834713.36it/s]\u001b[A\n",
            "36827915it [00:34, 967327.74it/s]\u001b[A\n",
            "36959348it [00:34, 1069444.96it/s]\u001b[A\n",
            "37091805it [00:34, 1144950.98it/s]\u001b[A\n",
            "37221212it [00:34, 1189251.41it/s]\u001b[A\n",
            "37355609it [00:34, 1235351.06it/s]\u001b[A\n",
            "37491541it [00:34, 1272356.90it/s]\u001b[A\n",
            "37627423it [00:34, 1298202.43it/s]\u001b[A\n",
            "37761047it [00:34, 1309575.82it/s]\u001b[A\n",
            "37893557it [00:34, 1314214.69it/s]\u001b[A\n",
            "38025068it [00:34, 1260533.92it/s]\u001b[A\n",
            "38160985it [00:35, 1289230.57it/s]\u001b[A\n",
            "38290365it [00:35, 1290078.75it/s]\u001b[A\n",
            "38419695it [00:35, 1259411.18it/s]\u001b[A\n",
            "38545986it [00:35, 1196587.27it/s]\u001b[A\n",
            "38666418it [00:35, 1183977.62it/s]\u001b[A^C\n"
          ]
        }
      ],
      "source": [
        "# 6. Executar Treinamento\n",
        "import os\n",
        "\n",
        "# Configurar ambiente\n",
        "os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + \":\" + os.path.join(os.getcwd(), 'splade')\n",
        "os.environ['SPLADE_CONFIG_NAME'] = \"config_splade_pt.yaml\"\n",
        "\n",
        "print(\"üöÄ Iniciando treinamento... Acompanhe os logs abaixo.\")\n",
        "print(\"Nota: Ignorar avisos 'Unable to register cuFFT/cuDNN' do TensorFlow/JAX.\")\n",
        "\n",
        "!cd splade && python3 -m splade.train_from_triplets_ids \\\n",
        "  config.checkpoint_dir=experiments/pt/checkpoint \\\n",
        "  config.index_dir=experiments/pt/index \\\n",
        "  config.out_dir=experiments/pt/out"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}